---
title: "Final Project"
subtitle: "Predicting Property Prices"
author: "Group 2"
date: "5/16/2021"
output:

  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
---

**Group 2 members:** _Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# Load the Ames Housing dataset.
url = 'https://raw.githubusercontent.com/schoolkidrich/housing_price/main/train.csv'
prices = read.csv(url, stringsAsFactors = TRUE)

# Packages.
library(tidyverse)
library(mice)
library(kableExtra)
library(reshape2)
```

## Abstract


## Key Words
Real Estate, House Prices, Multiple Regression, Assessed Value, Home Buyers, Linear Models.

## Introduction
The aim of this study is to explore the factors that influence home buyers when buying homes such as location, property size, age, number of rooms, etc, and how these factors affect house prices. In order to explore this topic, we used the "Ames Housing" dataset.


## Literature Review
[Understanding Recent Trends in House Prices and Home Ownership](https://www.kansascityfed.org/documents/3224/pdf-Shiller_0415.pdf) by Robert J. Shiller investigates some of the factors that affect housing booms. As well as physical factors influencing housing prices, Shiller argues that there are also psychological factors at play such as society's insistence that housing is an important investment.

[Cracking the Ames Housing Dataset with Linear Regression](https://towardsdatascience.com/wrangling-through-dataland-modeling-house-prices-in-ames-iowa-75b9b4086c96) by Alvin T. Tan Investigates the Ames Housing dataset from the point of view of the hedonic pricing regression technique. According to the [Organisation For Economic Co-Operation And Development](https://stats.oecd.org/glossary/detail.asp?ID=1225), the hedonic method is "A regression technique in which observed prices of different qualities or models of the same generic good or service are expressed as a function of the characteristics of the goods or services in question. It is based on the hypothesis that products can be treated as bundles of characteristics and that prices can be attached to the characteristics". Basically, items can be broken into their constituent parts and used to predict the target value based on how much influence those parts bear.

## Methodology

## Experimentation and Results:

### Data Exploration

#### Dataset
The Ames Housing dataset consists of 81 variables describing the characteristics of 1,460 homes in Ames, Iowa sold between 2006 and 2010. The dataset is available for download via the [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) website. The Ames Housing dataset is feature rich, and contains many of the features that home buyers consider when buying a house such as overall condition, location, number of rooms, etc. Below is a summary of the variables contained within the dataset.

* PID: Parcel identification number - can be used with city web site for parcel review.
* MS SubClass: Identifies the type of dwelling involved in the sale.
* MS Zoning: Identifies the general zoning classification of the sale.
* Lot Frontage: Linear feet of street connected to property
* Lot Area: Lot size in square feet
* Street: Type of road access to property
* Alley: Type of alley access to property
* Lot Shape: General shape of property
* Land Contour: Flatness of the property
* Utilities: Type of utilities available
* Lot Config: Lot configuration
* Land Slope: Slope of property
* Neighborhood: Physical locations within Ames city limits (map available)
* Condition 1: Proximity to various conditions
* Condition 2: Proximity to various conditions (if more than one is present)
* Bldg Type: Type of dwelling
* House Style: Style of dwelling
* Overall Qual: Rates the overall material and finish of the house
* Overall Cond: Rates the overall condition of the house
* Year Built: Original construction date
* Year Remod/Add: Remodel date (same as construction date if no remodeling or additions)
* Roof Style: Type of roof
* Roof Matl: Roof material
* Exterior 1: Exterior covering on house
* Exterior 2: Exterior covering on house (if more than one material)
* Mas Vnr Type: Masonry veneer type
* Mas Vnr Area: Masonry veneer area in square feet
* Exter Qual: Evaluates the quality of the material on the exterior
* Exter Cond: Evaluates the present condition of the material on the exterior
* Foundation: Type of foundation
* Bsmt Qual: Evaluates the height of the basement
* Bsmt Cond: Evaluates the general condition of the basement
* Bsmt Exposure: Refers to walkout or garden level walls
* BsmtFin Type 1: Rating of basement finished area
* BsmtFin SF 1: Type 1 finished square feet
* BsmtFinType 2: Rating of basement finished area (if multiple types)
* BsmtFin SF 2: Type 2 finished square feet
* Bsmt Unf SF: Unfinished square feet of basement area
* Total Bsmt SF: Total square feet of basement area
* Heating: Type of heating
* HeatingQC: Heating quality and condition
* Central Air: Central air conditioning
* Electrical: Electrical system
* 1st Flr SF: First Floor square feet
* 2nd Flr SF: Second floor square feet
* Low Qual Fin SF: Low quality finished square feet (all floors)
* Gr Liv Area: Above grade (ground) living area square feet
* Bsmt Full Bath: Basement full bathrooms
* Bsmt Half Bath: Basement half bathrooms
* Full Bath: Full bathrooms above grade
* Half Bath: Half baths above grade
* Bedroom: Bedrooms above grade (does NOT include basement bedrooms)
* Kitchen: Kitchens above grade
* KitchenQual: Kitchen quality
* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
* Functional: Home functionality (Assume typical unless deductions are warranted)
* Fireplaces: Number of fireplaces
* FireplaceQu: Fireplace quality
* Garage Type: Garage location
* Garage Yr Blt: Year garage was built
* Garage Finish: Interior finish of the garage
* Garage Cars: Size of garage in car capacity
* Garage Area: Size of garage in square feet
* Garage Qual: Garage quality
* Garage Cond: Garage condition
* Paved Drive: Paved driveway
* Wood Deck SF: Wood deck area in square feet
* Open Porch SF: Open porch area in square feet
* Enclosed Porch: Enclosed porch area in square feet
* 3-Ssn Porch: Three season porch area in square feet
* Screen Porch: Screen porch area in square feet
* Pool Area: Pool area in square feet
* Pool QC: Pool quality
* Fence: Fence quality
* Misc Feature: Miscellaneous feature not covered in other categories
* Misc Val: $Value of miscellaneous feature
* Mo Sold: Month Sold
* Yr Sold: Year Sold
* Sale Type: Type of sale
* Sale Condition: Condition of sale


#### Data Analysis
The first step in our data analysis was to get a feel for the data by creating bloxplots of all the variables. This help us to get an idea of the value of each variable, and whether or not a variable was worth keeping in the dataset.

#### Boxplots

``` {r boxPlots, warning=FALSE, message=FALSE, echo=FALSE}
# Plot boxplots for all variables.
long <- prices %>% as.data.frame() %>% melt()

long %>%
  ggplot(aes(x=value)) + geom_boxplot() + facet_wrap(~variable, scales = 'free')
```


#### Missing Values
We then checked for missing values. As we can see from the data summary below, there are quite a few columns with missing values (19 columns in all). The main offenders are at the top of the list.

```{r, dataExploration, echo=FALSE}
sapply(prices, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE) %>% kable() %>% kable_styling()
```


#### Data Preparation
After checking the data for missing values, we decided to drop columns consisting of more than 33% missing values. We did this in order to increase the accuracy of our final models. For the rest of the missing values, rather than removing them, we decided to impute them. The imputation was handled using the MICE package (Multiple Imputation by Chained Equations). This technique uses chained equations, and thus has the ability to impute both numerical and categorical variables.

```{r dataCleaning, warning=FALSE, message=FALSE, echo=FALSE}
# Drop columns consisting of more than 33% NAs.
col_names = names(prices)
na_count = map(prices, ~sum(is.na(.)))
num_cases = dim(prices)[1]

drop = c()
for (name in col_names){
  if (na_count[name] > (num_cases / 3)){
    drop = c(drop, name)
  }
}
```


We also noticed that there was a distinct lack of diversity in the Utilities column and thus we decided to drop this column. The ID column was also dropped due to the simple fact that it added no value to our final models.

```{r utilitiesAnalysis, warning=FALSE, message=FALSE, echo=FALSE}
prices %>% 
  ggplot(aes(x = Utilities)) + geom_bar()
```



```{r, dataManipulation}
# Drop Id column.
drop = c(drop, "Id", 'Utilities')
dropped = prices[, !(names(prices) %in% drop)]

imputed = dropped %>%
  # Impute missing values.
  type.convert(.) %>%
  mice(m = 5, meth = 'cart') %>%
  complete()
```



#### Model Building

```{r}
# 70% training set
train = sample(seq(num_cases),size = round(num_cases*.7))
train_set = imputed[train,]
test_set = imputed[-train,]


model1 = lm(SalePrice~., train_set)
summary(model1)

```

# model 2

```{r}
# using features with only numeric values
numeric_df <- train_set %>% dplyr::select(where(is.numeric))
model2 <- lm(SalePrice~., numeric_df)
summary(model2)
```

```{r}
# looking at residuals
ggplot(data = model2, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE) + xlab("Fitted values") + ylab("Residuals")

ggplot(data = model2, aes(x = .resid)) + geom_histogram() + xlab("Residuals")

ggplot(data = model2) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```

#### Model Selection

#### Model Evaluation


## Discussion and Conclusions

## References

## Appendices
