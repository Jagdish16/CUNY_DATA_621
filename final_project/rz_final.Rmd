---
title: "Final Project"
subtitle: "Predicting Property Prices"
author: "Group 2"
date: "5/16/2021"
output:

  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
---

**Group 2 members:** _Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# Load the Ames Housing dataset.
url = 'https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/final_project/train.csv'
prices = read.csv(url, stringsAsFactors = TRUE)

# Packages.
library(tidyverse)
library(mice)
library(kableExtra)
library(reshape2)
```

## Abstract


## Key Words
Real Estate, House Prices, Multiple Regression, Assessed Value, Home Buyers, Linear Models.

## Introduction
The aim of this study is to explore the factors that influence home buyers when buying homes such as location, property size, age, number of rooms, etc, and how these factors affect house prices. In order to explore this topic, we used the "Ames Housing" dataset.


## Literature Review
[Understanding Recent Trends in House Prices and Home Ownership](https://www.kansascityfed.org/documents/3224/pdf-Shiller_0415.pdf) by Robert J. Shiller investigates some of the factors that affect housing booms. As well as physical factors influencing housing prices, Shiller argues that there are also psychological factors at play such as society's insistence that housing is an important investment.

[Cracking the Ames Housing Dataset with Linear Regression](https://towardsdatascience.com/wrangling-through-dataland-modeling-house-prices-in-ames-iowa-75b9b4086c96) by Alvin T. Tan Investigates the Ames Housing dataset from the point of view of the hedonic pricing regression technique. According to the [Organisation For Economic Co-Operation And Development](https://stats.oecd.org/glossary/detail.asp?ID=1225), the hedonic method is "A regression technique in which observed prices of different qualities or models of the same generic good or service are expressed as a function of the characteristics of the goods or services in question. It is based on the hypothesis that products can be treated as bundles of characteristics and that prices can be attached to the characteristics". Basically, items can be broken into their constituent parts and used to predict the target value based on how much influence those parts bear.

## Methodology

In our investigation to predict house prices, we are given eighty-one categorical and numerical independent variables to use in a multiple linear regression model $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{\beta}_1 X_n + \hat{\epsilon}_i$.  

We begin our exploratory analysis by taking a look into all the variables in our data set. We take a glimpse into how the data looks like within the variables in the data set.  From there, we identify the categorical and numerical variables and the number of missing values by variable.  Next in our data exploration, we visualize the the variables and its distribution using boxplots. Lastly, we illustrate the correlation of our independent variables to our dependent variable, house prices.

The preparation for the data set only includes the imputation of our missing values using classification regression tree.  The data set used is otherwise clean -- variables' datatypes are properly stored.

In creating a multiple linear models, we begin with our first model that includes all eighty independent variables to predict the house prices.  The next model created is the model that only includes the numerical values, ignoring the categorical independent variables. LAST MODEL MISSING.


The four assumptions used in our model are: 1) Residuals of the model are nearly normal. 2) Variability of the residuals is nearly constant. 3) Residuals are independent. 4) Each variable is linearly related to the outcome.  We evaluate these assumptions through plotting the residuals in a quantile-quantile (q-q) plot, distribution plot, and by taking the absolute values against the fitted values to determine that the variability and distribution are normal.

In choosing the best fitting multiple linear regression model, we prioritze the model's adjusted R-squared values and the variability of the residuals.

## Experimentation and Results:

### Data Exploration

#### Dataset
The Ames Housing dataset consists of 81 variables describing the characteristics of 1,460 homes in Ames, Iowa sold between 2006 and 2010. The dataset is available for download via the [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) website. The Ames Housing dataset is feature rich, and contains many of the features that home buyers consider when buying a house such as overall condition, location, number of rooms, etc. Below is a summary of the variables contained within the dataset.

* PID: Parcel identification number - can be used with city web site for parcel review.
* MS SubClass: Identifies the type of dwelling involved in the sale.
* MS Zoning: Identifies the general zoning classification of the sale.
* Lot Frontage: Linear feet of street connected to property
* Lot Area: Lot size in square feet
* Street: Type of road access to property
* Alley: Type of alley access to property
* Lot Shape: General shape of property
* Land Contour: Flatness of the property
* Utilities: Type of utilities available
* Lot Config: Lot configuration
* Land Slope: Slope of property
* Neighborhood: Physical locations within Ames city limits (map available)
* Condition 1: Proximity to various conditions
* Condition 2: Proximity to various conditions (if more than one is present)
* Bldg Type: Type of dwelling
* House Style: Style of dwelling
* Overall Qual: Rates the overall material and finish of the house
* Overall Cond: Rates the overall condition of the house
* Year Built: Original construction date
* Year Remod/Add: Remodel date (same as construction date if no remodeling or additions)
* Roof Style: Type of roof
* Roof Matl: Roof material
* Exterior 1: Exterior covering on house
* Exterior 2: Exterior covering on house (if more than one material)
* Mas Vnr Type: Masonry veneer type
* Mas Vnr Area: Masonry veneer area in square feet
* Exter Qual: Evaluates the quality of the material on the exterior
* Exter Cond: Evaluates the present condition of the material on the exterior
* Foundation: Type of foundation
* Bsmt Qual: Evaluates the height of the basement
* Bsmt Cond: Evaluates the general condition of the basement
* Bsmt Exposure: Refers to walkout or garden level walls
* BsmtFin Type 1: Rating of basement finished area
* BsmtFin SF 1: Type 1 finished square feet
* BsmtFinType 2: Rating of basement finished area (if multiple types)
* BsmtFin SF 2: Type 2 finished square feet
* Bsmt Unf SF: Unfinished square feet of basement area
* Total Bsmt SF: Total square feet of basement area
* Heating: Type of heating
* HeatingQC: Heating quality and condition
* Central Air: Central air conditioning
* Electrical: Electrical system
* 1st Flr SF: First Floor square feet
* 2nd Flr SF: Second floor square feet
* Low Qual Fin SF: Low quality finished square feet (all floors)
* Gr Liv Area: Above grade (ground) living area square feet
* Bsmt Full Bath: Basement full bathrooms
* Bsmt Half Bath: Basement half bathrooms
* Full Bath: Full bathrooms above grade
* Half Bath: Half baths above grade
* Bedroom: Bedrooms above grade (does NOT include basement bedrooms)
* Kitchen: Kitchens above grade
* KitchenQual: Kitchen quality
* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
* Functional: Home functionality (Assume typical unless deductions are warranted)
* Fireplaces: Number of fireplaces
* FireplaceQu: Fireplace quality
* Garage Type: Garage location
* Garage Yr Blt: Year garage was built
* Garage Finish: Interior finish of the garage
* Garage Cars: Size of garage in car capacity
* Garage Area: Size of garage in square feet
* Garage Qual: Garage quality
* Garage Cond: Garage condition
* Paved Drive: Paved driveway
* Wood Deck SF: Wood deck area in square feet
* Open Porch SF: Open porch area in square feet
* Enclosed Porch: Enclosed porch area in square feet
* 3-Ssn Porch: Three season porch area in square feet
* Screen Porch: Screen porch area in square feet
* Pool Area: Pool area in square feet
* Pool QC: Pool quality
* Fence: Fence quality
* Misc Feature: Miscellaneous feature not covered in other categories
* Misc Val: $Value of miscellaneous feature
* Mo Sold: Month Sold
* Yr Sold: Year Sold
* Sale Type: Type of sale
* Sale Condition: Condition of sale


#### Data Analysis
The first step in our data analysis is to get a feel for the data by generating a glimpse of the dataset. As we can see from the below results, the 81 variables contained within the dataset are a mixture of integer, and factor variables.

``` {r, dataGlimpse, warning=FALSE, message=FALSE, echo=FALSE}
glimpse(prices)
```

#### Missing Values
The dataset glimpse results above also reveal something else to us - many of the columns contain missing values which could be problematic when it comes to generating our models. This deserves further investigation, so we will now hone in on these columns to get an idea of the quantity of missing values contained within each column. The table below represents a count of missing values per column in descending order.

```{r, dataExploration, echo=FALSE}
sapply(prices, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE) %>% kable() %>% kable_styling()
```

### Data Preparation
#### Data Imputation
The first order of business when it comes to data preparation is to deal with the missing values in the data. Looking at the table above, it appears that there are quite a few columns containing NA values (19 columns in all). However, according to the [AMES dataset description](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt), some variables contain genuine NA values that have meaning within the context of the data. For example, an "NA" value in the Alley column represents "No alley access", an "NA" value within the "BsmtQual" column represents "No Basement", and so on. Therefore, to prevent these from being interpreted as true empty NA values, we imputed them to have more meaningful values (i.e. NoAlleyAccess, NoBasement), and ran the empty values check again.

```{r, imputeGenuineNAValues, warning=FALSE, message=FALSE, echo=FALSE}
# Replace genuine NA values with more meaningful values. 
na_replace <- function(dataframe) {
dataframe %>%
   mutate(Alley = fct_explicit_na(Alley, na_level = 'NoAlleyAccess'),
          BsmtQual = fct_explicit_na(BsmtQual, na_level = 'NoBasement'),
          BsmtCond = fct_explicit_na(BsmtCond, na_level = 'NoBasement'),
          BsmtExposure = fct_explicit_na(BsmtExposure, na_level = 'NoBasement'),
          BsmtFinType1 = fct_explicit_na(BsmtFinType1, na_level = 'NoBasement'),
          BsmtFinType2 = fct_explicit_na(BsmtFinType2, na_level = 'NoBasement'),
          FireplaceQu = fct_explicit_na(FireplaceQu, na_level = 'NoFireplace'),
          GarageType = fct_explicit_na(GarageType, na_level = 'NoGarage'),
          GarageFinish = fct_explicit_na(GarageFinish, na_level = 'NoGarage'),
          GarageQual = fct_explicit_na(GarageQual, na_level = 'NoGarage'),
          GarageCond = fct_explicit_na(GarageCond, na_level = 'NoGarage'),
          PoolQC = fct_explicit_na(PoolQC, na_level = 'NoPool'),
          Fence = fct_explicit_na(Fence, na_level = 'NoFence'),
          MiscFeature = fct_explicit_na(MiscFeature, na_level = 'None')
   )
}

prices <- na_replace(prices)

# Check for empty values once again to see what affect this has on the data.
sapply(prices, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE) %>% kable() %>% kable_styling()
```

On imputation of all of the genuine NA values in the dataset and re-counting, we can see that the top offending variables are no longer listed as having missing values - great news. However, we are still left with 38 variables that contain missing values, so our next order of business is to deal with these variables.

There are several imputation options available to us at this point. We can do nothing (*not a lot of help as this will hinder the quality of our models*), remove observations that contain missing values (*better to avoid this option if possible as it may affect the accuracy of our results*), use Multivariate Imputation by Chained Equation (MICE), k-nearest neighbors, or impute using mean/median values.

Taking a look at the Glimpse report we generated at the begining of our study, our dataset consists of both numerical and categorical variables. For this reason, the **MICE** imputation method would appear to be our best option as it deals with both numerical and categorical variables.

After running the MICE algorithm on our dataset and re-running the empty values check, we were left with zero missing values (*as reflected in the table below*).

```{r, miceImputation, warning=FALSE, message=FALSE, echo=FALSE}
# Impute missing values using the MICE algorithm.
mice_imputation <- function(dataframe) {
  imputation <- mice(dataframe, m = 1, method = 'cart')
  imputed <- mice::complete(imputation)
}

prices <- mice_imputation(prices)

# Check for empty values once again to see what affect MICE had on our data.
sapply(imputed, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE) %>% kable() %>% kable_styling()
```












#### Boxplots
Now that we have an understanding of the variable types, we can create boxplots to get a visual representation of the data. This will help us to get an idea of the value of each variable, and whether or not specific variables should be dropped from the dataset.

``` {r boxPlots, warning=FALSE, message=FALSE, echo=FALSE}
# Plot boxplots for all variables.
long <- prices %>% as.data.frame() %>% melt()

long %>%
  ggplot(aes(x=value)) + geom_boxplot() + facet_wrap(~variable, scales = 'free')
```



#### Data Preparation
After checking the data for missing values, we decided to drop columns consisting of more than 33% missing values. We did this in order to increase the accuracy of our final models. For the rest of the missing values, rather than removing them, we decided to impute them. The imputation was handled using the MICE package (Multiple Imputation by Chained Equations). This technique uses chained equations, and thus has the ability to impute both numerical and categorical variables.

```{r dataCleaning, warning=FALSE, message=FALSE, echo=FALSE}
# Drop columns consisting of more than 33% NAs.
#col_names = names(prices)
#na_count = map(prices, ~sum(is.na(.)))
#num_cases = dim(prices)[1]

#drop = c()
#for (name in col_names){
#  if (na_count[name] > (num_cases / 3)){
#    drop = c(drop, name)
#  }
#}
```


We also noticed that there was a distinct lack of diversity in the Utilities column and thus we decided to drop this column. The ID column was also dropped due to the simple fact that it added no value to our final models.

```{r utilitiesAnalysis, warning=FALSE, message=FALSE, echo=FALSE}
prices %>% 
  ggplot(aes(x = Utilities)) + geom_bar()
```



```{r, dataManipulation}
# Drop Id column.
#drop = c(drop, "Id", 'Utilities')
#dropped = prices[, !(names(prices) %in% drop)]

#imputed = dropped %>%
  # Impute missing values.
#  type.convert(.) %>%
#  mice(m = 5, meth = 'cart') %>%
#  complete()
```



#### Model Building

```{r}
# 70% training set
train = sample(seq(num_cases), size = round(num_cases * .7))
train_set = imputed[train,]
test_set = imputed[-train,]


model1 = lm(SalePrice~., train_set)
summary(model1)

```

# model 2

```{r}
# using features with only numeric values
numeric_df <- train_set %>% dplyr::select(where(is.numeric))
model2 <- lm(SalePrice~., numeric_df)
summary(model2)
```

```{r}
# looking at residuals
ggplot(data = model2, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE) + xlab("Fitted values") + ylab("Residuals")

ggplot(data = model2, aes(x = .resid)) + geom_histogram() + xlab("Residuals")

ggplot(data = model2) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```

#### Model Selection

#### Model Evaluation


## Discussion and Conclusions

## References

## Appendices
