---
title: "Final Project"
subtitle: "Predicting Property Prices"
author: "Group 2"
date: "5/16/2021"
output:

  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
---

**Group 2 members:** _Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# Load the Ames Housing dataset.
url = 'https://raw.githubusercontent.com/schoolkidrich/housing_price/main/train.csv'
prices = read.csv(url, stringsAsFactors = TRUE)

# Packages.
library(tidyverse)
library(mice)
```

## Abstract


## Key Words

Real Estate, House Prices, Multiple Regression, Assessed Value, Home Buyers, Linear Models.

## Introduction
The aim of this study is to explore the factors that influence home buyers when buying homes such as location, property size, age, number of rooms, etc, and how these factors affect house prices. In order to explore this topic, we used the "Ames Housing" dataset.


# Literature Review

# Methodology

## Data Exploration

### Dataset
The Ames Housing dataset consists of 81 variables describing the characteristics of 1,460 homes in Ames, Iowa sold between 2006 and 2010. The dataset is available for download via the [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) website. The Ames Housing dataset is feature rich, and contains many of the features that home buyers consider when buying a house such as overall condition, location, number of rooms etc.  

### Data Description

### Data Analysis
The first step in our data analysis was to check for missing values. As you can see from the below summary there are quite of few columns with missing values. The main offenders are at the top of the list ()

```{r}
# each row represents one house 
num_cases = dim(prices)[1]
print(paste("There are",num_cases,"cases"))
```


# Notice that there are missing data

```{r}
summary(prices)
```

# cleaning data

```{r}
# drop columns with more than 33% as NA
col_names = names(prices)
na_count = map(prices, ~sum(is.na(.)))

drop = c()
for (name in col_names){
  if (na_count[name] > (num_cases/3)){
    drop = c(drop,name)
  }
}
```

# Boxplots

``` {r boxPlots, warning=FALSE, message=FALSE, echo=FALSE}
# Plot boxplots for all variables.
long <- prices %>% as.data.frame() %>% melt()

long %>%
  ggplot(aes(x=value)) + geom_boxplot() + facet_wrap(~variable, scales = 'free')
```

# Need to drop Utilities column due to lack of diversity

```{r}
prices%>% 
  ggplot(aes(x=Utilities))+geom_bar()
```

# drop values and impute missing data

```{r}
# drop Id column
drop = c(drop, "Id", 'Utilities')
dropped = prices[,!(names(prices) %in% drop)]

imputed = dropped%>%
  # impute missing values
  type.convert(.)%>%
  mice(m=5,meth='cart')%>%
  complete()



```



# building model

```{r}
# 70% training set
train = sample(seq(num_cases),size = round(num_cases*.7))
train_set = imputed[train,]
test_set = imputed[-train,]


model1 = lm(SalePrice~., train_set)
summary(model1)

```

# model 2

```{r}
# using features with only numeric values
numeric_df <- train_set %>% dplyr::select(where(is.numeric))
model2 <- lm(SalePrice~., numeric_df)
summary(model2)
```

```{r}
# looking at residuals
ggplot(data = model2, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE) + xlab("Fitted values") + ylab("Residuals")

ggplot(data = model_two, aes(x = .resid)) + geom_histogram() + xlab("Residuals")

ggplot(data = model_two) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```


