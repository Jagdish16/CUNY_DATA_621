---
title: "Final Project"
subtitle: "Predicting Property Prices"
author: "Group 2"
date: "5/16/2021"
output:

  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
---

**Group 2 members:** _Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE, echo=TRUE, results='hide', warning=FALSE, message=FALSE}
# Load the Ames Housing dataset.
url = 'https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/final_project/train.csv'
prices = read.csv(url, stringsAsFactors = TRUE)
prices.test = read.csv('https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/final_project/test.csv')
# Packages.
library(tidyverse)
library(mice)
library(kableExtra)
library(reshape2)
library(Hmisc)
```

## Abstract


## Key Words
Real Estate, House Prices, Multiple Regression, Assessed Value, Home Buyers, Linear Models.

## Introduction
The aim of this study is to explore the factors that influence home buyers when buying homes such as location, property size, age, number of rooms, etc, and how these factors affect house prices. In order to explore this topic, we used the "Ames Housing" dataset.


## Literature Review
[Understanding Recent Trends in House Prices and Home Ownership](https://www.kansascityfed.org/documents/3224/pdf-Shiller_0415.pdf) by Robert J. Shiller investigates some of the factors that affect housing booms. As well as physical factors influencing housing prices, Shiller argues that there are also psychological factors at play such as society's insistence that housing is an important investment.

[Cracking the Ames Housing Dataset with Linear Regression](https://towardsdatascience.com/wrangling-through-dataland-modeling-house-prices-in-ames-iowa-75b9b4086c96) by Alvin T. Tan Investigates the Ames Housing dataset from the point of view of the hedonic pricing regression technique. According to the [Organisation For Economic Co-Operation And Development](https://stats.oecd.org/glossary/detail.asp?ID=1225), the hedonic method is "A regression technique in which observed prices of different qualities or models of the same generic good or service are expressed as a function of the characteristics of the goods or services in question. It is based on the hypothesis that products can be treated as bundles of characteristics and that prices can be attached to the characteristics". Basically, items can be broken into their constituent parts and used to predict the target value based on how much influence those parts bear.

## Methodology

In our investigation to predict house prices, we are given eighty-one categorical and numerical independent variables to use in a multiple linear regression model $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{\beta}_1 X_n + \hat{\epsilon}_i$.  

We begin our exploratory analysis by taking a look into all the variables in our data set. We take a glimpse into how the data looks like within the variables in the data set.  From there, we identify the categorical and numerical variables and the number of missing values by variable.  Next in our data exploration, we visualize the the variables and its distribution using boxplots. Lastly, we illustrate the correlation of our independent variables to our dependent variable, house prices.

The preparation for the data set only includes the imputation of our missing values using classification regression tree.  The data set used is otherwise clean -- variables' datatypes are properly stored.

In creating a multiple linear models, we begin with our first model that includes all eighty independent variables to predict the house prices.  The next model created is the model that only includes the numerical values, ignoring the categorical independent variables. LAST MODEL MISSING.


The four assumptions used in our model are: 1) Residuals of the model are nearly normal. 2) Variability of the residuals is nearly constant. 3) Residuals are independent. 4) Each variable is linearly related to the outcome.  We evaluate these assumptions through plotting the residuals in a quantile-quantile (q-q) plot, distribution plot, and by taking the absolute values against the fitted values to determine that the variability and distribution are normal.

In choosing the best fitting multiple linear regression model, we prioritze the model's adjusted R-squared values and the variability of the residuals.

## Experimentation and Results:

### Data Exploration

#### Dataset
The Ames Housing dataset consists of 81 variables describing the characteristics of 1,460 homes in Ames, Iowa sold between 2006 and 2010. The dataset is available for download via the [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) website. The Ames Housing dataset is feature rich, and contains many of the features that home buyers consider when buying a house such as overall condition, location, number of rooms, etc. Below is a summary of the variables contained within the dataset.

* PID: Parcel identification number - can be used with city web site for parcel review.
* MS SubClass: Identifies the type of dwelling involved in the sale.
* MS Zoning: Identifies the general zoning classification of the sale.
* Lot Frontage: Linear feet of street connected to property
* Lot Area: Lot size in square feet
* Street: Type of road access to property
* Alley: Type of alley access to property
* Lot Shape: General shape of property
* Land Contour: Flatness of the property
* Utilities: Type of utilities available
* Lot Config: Lot configuration
* Land Slope: Slope of property
* Neighborhood: Physical locations within Ames city limits (map available)
* Condition 1: Proximity to various conditions
* Condition 2: Proximity to various conditions (if more than one is present)
* Bldg Type: Type of dwelling
* House Style: Style of dwelling
* Overall Qual: Rates the overall material and finish of the house
* Overall Cond: Rates the overall condition of the house
* Year Built: Original construction date
* Year Remod/Add: Remodel date (same as construction date if no remodeling or additions)
* Roof Style: Type of roof
* Roof Matl: Roof material
* Exterior 1: Exterior covering on house
* Exterior 2: Exterior covering on house (if more than one material)
* Mas Vnr Type: Masonry veneer type
* Mas Vnr Area: Masonry veneer area in square feet
* Exter Qual: Evaluates the quality of the material on the exterior
* Exter Cond: Evaluates the present condition of the material on the exterior
* Foundation: Type of foundation
* Bsmt Qual: Evaluates the height of the basement
* Bsmt Cond: Evaluates the general condition of the basement
* Bsmt Exposure: Refers to walkout or garden level walls
* BsmtFin Type 1: Rating of basement finished area
* BsmtFin SF 1: Type 1 finished square feet
* BsmtFinType 2: Rating of basement finished area (if multiple types)
* BsmtFin SF 2: Type 2 finished square feet
* Bsmt Unf SF: Unfinished square feet of basement area
* Total Bsmt SF: Total square feet of basement area
* Heating: Type of heating
* HeatingQC: Heating quality and condition
* Central Air: Central air conditioning
* Electrical: Electrical system
* 1st Flr SF: First Floor square feet
* 2nd Flr SF: Second floor square feet
* Low Qual Fin SF: Low quality finished square feet (all floors)
* Gr Liv Area: Above grade (ground) living area square feet
* Bsmt Full Bath: Basement full bathrooms
* Bsmt Half Bath: Basement half bathrooms
* Full Bath: Full bathrooms above grade
* Half Bath: Half baths above grade
* Bedroom: Bedrooms above grade (does NOT include basement bedrooms)
* Kitchen: Kitchens above grade
* KitchenQual: Kitchen quality
* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
* Functional: Home functionality (Assume typical unless deductions are warranted)
* Fireplaces: Number of fireplaces
* FireplaceQu: Fireplace quality
* Garage Type: Garage location
* Garage Yr Blt: Year garage was built
* Garage Finish: Interior finish of the garage
* Garage Cars: Size of garage in car capacity
* Garage Area: Size of garage in square feet
* Garage Qual: Garage quality
* Garage Cond: Garage condition
* Paved Drive: Paved driveway
* Wood Deck SF: Wood deck area in square feet
* Open Porch SF: Open porch area in square feet
* Enclosed Porch: Enclosed porch area in square feet
* 3-Ssn Porch: Three season porch area in square feet
* Screen Porch: Screen porch area in square feet
* Pool Area: Pool area in square feet
* Pool QC: Pool quality
* Fence: Fence quality
* Misc Feature: Miscellaneous feature not covered in other categories
* Misc Val: $Value of miscellaneous feature
* Mo Sold: Month Sold
* Yr Sold: Year Sold
* Sale Type: Type of sale
* Sale Condition: Condition of sale


#### Data Analysis
The first step in our data analysis is to get a feel for the data by generating a glimpse of the dataset. As we can see from the below results, the 81 variables contained within the dataset are a mixture of integer, and factor variables.

``` {r, dataGlimpse, warning=FALSE, message=FALSE, echo=FALSE}
glimpse(prices)
```

#### Missing Values
The dataset glimpse results above also reveal something else to us - many of the columns contain missing values which could be problematic when it comes to generating our models. This deserves further investigation, so we will now hone in on these columns to get an idea of the quantity of missing values contained within each column. The table below represents a count of missing values per column in descending order.

```{r, dataExploration, echo=FALSE}
sapply(prices, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE) %>% kable() %>% kable_styling()
```

### Data Preparation
#### Data Imputation
The first order of business when it comes to data preparation is to deal with the missing values in the data. Looking at the table above, it appears that there are quite a few columns containing NA values (19 columns in all). However, according to the [AMES dataset description](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt), some variables contain genuine NA values that have meaning within the context of the data. For example, an "NA" value in the Alley column represents "No alley access", an "NA" value within the "BsmtQual" column represents "No Basement", and so on. Therefore, to prevent these from being interpreted as true empty NA values, we imputed them to have more meaningful values (i.e. NoAlleyAccess, NoBasement), and ran the empty values check again.

```{r, imputeGenuineNAValues, warning=FALSE, message=FALSE, echo=FALSE}
#' na_replace - NA Replace.
#'
#' Given the Ames Housing dataset, converts genuine NA values that have
#' meaning within the context of the data to more meaningful values, and
#' returns the altered dataset to illiminate the mistaken interpretation
#' of the term "NA" as a genuine missing value.
#'
#' @param dataframe The Ames dataset as a dataframe.
#'
#' @return The Ames dataset with genuine NA values imputed to human friendly values.
na_replace <- function(dataframe) {
  dataframe %>%
    mutate(Alley = fct_explicit_na(Alley, na_level = 'NoAlleyAccess'),
           BsmtQual = fct_explicit_na(BsmtQual, na_level = 'NoBasement'),
           BsmtCond = fct_explicit_na(BsmtCond, na_level = 'NoBasement'),
           BsmtExposure = fct_explicit_na(BsmtExposure, na_level = 'NoBasement'),
           BsmtFinType1 = fct_explicit_na(BsmtFinType1, na_level = 'NoBasement'),
           BsmtFinType2 = fct_explicit_na(BsmtFinType2, na_level = 'NoBasement'),
           FireplaceQu = fct_explicit_na(FireplaceQu, na_level = 'NoFireplace'),
           GarageType = fct_explicit_na(GarageType, na_level = 'NoGarage'),
           GarageFinish = fct_explicit_na(GarageFinish, na_level = 'NoGarage'),
           GarageQual = fct_explicit_na(GarageQual, na_level = 'NoGarage'),
           GarageCond = fct_explicit_na(GarageCond, na_level = 'NoGarage'),
           PoolQC = fct_explicit_na(PoolQC, na_level = 'NoPool'),
           Fence = fct_explicit_na(Fence, na_level = 'NoFence'),
           MiscFeature = fct_explicit_na(MiscFeature, na_level = 'None')
    )
}
prices <- na_replace(prices)
# Check for empty values once again to see what affect this has on the data.
sapply(prices, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE) %>% kable() %>% kable_styling()
```

#### Correlations

```{r, Correlations, warning=FALSE, message=FALSE, echo=FALSE}
corr_data<- select_if(prices,is.numeric) %>%
  select(-Id) %>%
  as.matrix(.) %>%
  rcorr(.)
corr_p <- round(corr_data$P,4)

# this takes the values and correlations and makes it into a 2 column dataframe
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}
# sorted the pairs of correlations by their p value to show variables with the biggest
# relationships, with the p showing the significance value
flattenCorrMatrix(corr_data$r, corr_data$P) %>%
  arrange(desc(abs(cor))) %>%
  head(10) %>%
  kable(caption = 'Correlations of numeric predictors') %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

flattenCorrMatrix(corr_data$r, corr_data$P) %>%
  arrange(desc(abs(cor))) %>%
  filter(column == 'SalePrice') %>%
  head(10) %>%
  kable(caption = 'Correlations of numeric predictors against the Sales Price') %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```
Looking at the correlations, the amount of cars in the garage is correlated with garage area. It was
also expected that the year the house was built is highly correlated with the year the garage was built.
Similarly, the basement is correlated to the first floor's square footage. The more rooms a house has, the
greater the living area is. 

The overall quality of the material and finish of the house seems to have the greatest
affect on the sales price. The second variable to have a great affect is the total
square footage of the house, followed by the amount of cars in the garage and its
square footage.

#### Graphs

As shown by the graph, there is an outlier due to a house being much larger than 
all the other houses in the city. Overall, as the total square footage increase, so does
the sales price.

The sales price for each house tend to vary more as the overall quality increases. 
The overall condition of the house does not affect the sales price as one would expect
because houses with only a condition of 5 out of 10 tend to sell for more. Single
family homes also tend to vary in price compared to the other building types.

Homes that have amenities or special feautures such as central air conditioning and fireplaces
are more likely to sell for more.

```{r,Plots, warning=FALSE, message=FALSE, fig.show = "hold", out.width="33%", echo=FALSE}
prices %>%
  mutate(TotalSqFt = GrLivArea + TotalBsmtSF) %>%
  ggplot(., aes(x = TotalSqFt, y = SalePrice)) +
  geom_point() + 
  geom_smooth() +
  ggtitle("Total Square Footage vs Sales Price") +
  scale_y_continuous(labels = scales::label_comma())

prices %>%
  mutate(OverallQual= as.factor(OverallQual)) %>%
  ggplot(., aes(x = OverallQual, y = SalePrice)) + 
  geom_boxplot() +
  labs(title = 'Distributions of Overall Quality vs Sales Price') +
  scale_y_continuous(labels = scales::label_comma())

prices %>%
  mutate(OverallCond= as.factor(OverallCond)) %>%
  ggplot(., aes(x = OverallCond, y = SalePrice)) + 
  geom_boxplot() +
  labs(title = 'Distributions of Overall Condition vs Sales Price') +
  scale_y_continuous(labels = scales::label_comma())

prices %>%
  mutate(GarageCars = as.factor(GarageCars)) %>%
  ggplot(., aes(x = GarageCars, y = SalePrice)) + 
  geom_boxplot() +
  labs(title = 'Distributions of Amount of Cars in Garage vs Sales Price') +
  scale_y_continuous(labels = scales::label_comma())

prices %>%
  mutate(BldgType = recode(BldgType, `2fmCon` = "2Fam Conversion",
                           `Twnhs` = "Townhouse Inside",
                           `TwnhsE` = "Townhouse End Unit")) %>%
  ggplot(., aes(x = BldgType, y = SalePrice)) + 
  geom_boxplot() +
  labs(title = 'Type of Building vs Sales Price') +
  scale_y_continuous(labels = scales::label_comma())

prices %>%
  ggplot(., aes(x = CentralAir, y = SalePrice)) + 
  geom_boxplot() +
  labs(title = 'Central Air Coniditioning vs Sales Price') +
  scale_y_continuous(labels = scales::label_comma())

prices %>%
  mutate(Fireplaces = ifelse(Fireplaces == 0, "no", "yes")) %>%
  ggplot(., aes(x = Fireplaces, y = SalePrice)) + 
  geom_boxplot() +
  labs(title = 'Fireplaces vs Sales Price') +
  scale_y_continuous(labels = scales::label_comma())

```

##### Dropping values

```{r, DroppingValues, echo = FALSE}
drop = c(drop, "Id",'Condition1', 'Condition2', 'RoofStyle')
dropped = prices[,!(names(prices) %in% drop)]
```


On imputation of all of the genuine NA values in the dataset and re-counting, we can see that the top offending variables are no longer listed as having missing values - great news. However, we are still left with 38 variables that contain missing values, so our next order of business is to deal with these variables.

There are several imputation options available to us at this point. We can do nothing (*not a lot of help as this will hinder the quality of our models*), remove observations that contain missing values (*better to avoid this option if possible as it may affect the accuracy of our results*), use Multivariate Imputation by Chained Equation (MICE), k-nearest neighbors, or impute using mean/median values.

Taking a look at the Glimpse report we generated at the begining of our study, our dataset consists of both numerical and categorical variables. For this reason, the **MICE** imputation method would appear to be our best option as it deals with both numerical and categorical variables.

After running the MICE algorithm on our dataset and re-running the empty values check, we were left with zero missing values (*as reflected in the table below*).

```{r, miceImputation, warning=FALSE, message=FALSE, echo=FALSE}
#' mice_imputation- Mice Imputation.
#'
#' Given the Ames Housing dataset, runs the MICE algorithm on the dataset
#' to impute both numerical and categorical missing values.
#'
#' @param dataframe The Ames dataset as a dataframe.
#'
#' @return The Ames dataset with missing values imputed to complete values.
#'
mice_imputation <- function(dataframe) {
  imputation <- mice(dataframe, m = 1, method = 'cart')
  imputed <- mice::complete(imputation)
}
imputed <- mice_imputation(dropped)
# Check for empty values once again to see what affect MICE had on our data.
sapply(imputed, function(x) sum(is.na(x))) %>% sort(decreasing = TRUE) %>% kable() %>% kable_styling()
```


### Model Building
Now that we have a dataset that is free from empty values, we can move on to building our models.

#### Model One
```{r, modelOne, warning=FALSE, message=FALSE, echo=FALSE}
train_set = imputed
model1 = lm(SalePrice~., train_set)
summary(model1)
```

```{r, modelOneRisiduals,  warning=FALSE, message=FALSE, fig.show = "hold", out.width="33%", echo=FALSE}
ggplot(data = model1, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_smooth(se = FALSE) + xlab('Fitted values') + ylab('Residuals')
ggplot(data = model1, aes(x = .resid)) + geom_histogram() + xlab('Residuals')
ggplot(data = model1) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```

#### Model Two
Model two filters out non numeric values.
```{r, modelTwo, warning=FALSE, message=FALSE, echo=FALSE}
# Using features with only numeric values.
numeric_df <- train_set %>% dplyr::select(where(is.numeric))
model2 <- lm(SalePrice~., numeric_df)
summary(model2)
```

```{r, modelTwoRisiduals,  warning=FALSE, message=FALSE, fig.show = "hold", out.width="33%", echo=FALSE}
# Looking at residuals.
ggplot(data = model2, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE) + xlab("Fitted values") + ylab("Residuals")
ggplot(data = model2, aes(x = .resid)) + geom_histogram() + xlab("Residuals")
ggplot(data = model2) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```


#### Model Three
Model three utilizes model one, but uses backward stepwise elimination.
```{r, modelThree, warning=FALSE, message=FALSE, echo=FALSE}
model3 <- step(model1, direction = 'backward', trace = 0)
summary(model3)
```

```{r, modelThreeRisiduals,  warning=FALSE, message=FALSE, fig.show = "hold", out.width="33%", echo=FALSE}
ggplot(data = model3, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_smooth(se = FALSE) + xlab('Fitted values') + ylab('Residuals')
ggplot(data = model3, aes(x = .resid)) + geom_histogram() + xlab('Residuals')
ggplot(data = model3) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```

#### Model Four

Model four utilizes forward selection for the most part with recommendations from the
maker of the data set where he suggested to include the neighborhood in the model. 
Bathrooms were combined together as `TotalBath` and the age of the home is denoted by 
`Age`. `SaleCondition` was regrouped as normal and other. `NewHome` is whether or not the
house sold is new.

```{r, modelFour, warning=FALSE, message=FALSE, echo=FALSE}
transform = train_set %>%
  filter(GrLivArea < 4000) %>%
  mutate(TotalBath = BsmtFullBath + 0.5 * BsmtHalfBath + FullBath + 0.5 * HalfBath,
         Age = YrSold - YearBuilt,
         SaleCondition = ifelse(SaleCondition == "Normal", "normal", "other"),
         PorchSqFt = ScreenPorch + X3SsnPorch + EnclosedPorch + OpenPorchSF + WoodDeckSF,
         NewHome = ifelse(SaleType == 'New', 'new', 'other'))
model4 = lm(log(SalePrice) ~ GrLivArea + TotalBsmtSF + OverallQual + Neighborhood + NewHome +
     Age + CentralAir + Fireplaces + GarageArea + TotalBath  + PorchSqFt + PoolArea +
     SaleCondition + MSZoning + BldgType + OverallCond , data = transform)
summary(model4)
```

```{r, modelFourRisiduals,  warning=FALSE, message=FALSE, fig.show = "hold", out.width="33%", echo=FALSE}
ggplot(data = model4, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_smooth(se = FALSE) + xlab('Fitted values') + ylab('Residuals')
ggplot(data = model4, aes(x = .resid)) + geom_histogram() + xlab('Residuals')
ggplot(data = model4) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```

#### Model Selection

we selected model 4 due to it fitting the data well as its adjusted $R^2$ is the highest at 0.9143,
which is slightly more than model 3 with an adjusted $R^2$ of 0.9077, which was more 
computationally expensive. We think model 4 would be a better fit for a production environment
where such a model might be used, to predict housing prices on demand, as it is a great fit 
and computes reasonably fast. 

```{r}
# the model is suitable for linaer regression as residuals meet the following criteria 
# residuals are clustered around 0
ggplot(data = model4, aes(x = .fitted, y = .resid)) +
  geom_point() + geom_hline(yintercept = 0, linetype = 'dashed') +
  geom_smooth(se = FALSE) + xlab('Fitted values') + ylab('Residuals')
# residuals are normally distributed
ggplot(data = model4, aes(x = .resid)) + geom_histogram() + xlab('Residuals')
# qq plot also shows that residuals are almost normal
ggplot(data = model4) + stat_qq(aes(sample = .stdresid)) + geom_abline()
```


#### Model Evaluation

```{r}
test = prices.test[,!(names(prices.test) %in% drop)]
test.impute = test%>%
  na_replace()%>%
  mice_imputation%>%
  filter(GrLivArea < 4000) %>%
  mutate(TotalBath = BsmtFullBath + 0.5 * BsmtHalfBath + FullBath + 0.5 * HalfBath,
         Age = YrSold - YearBuilt,
         SaleCondition = ifelse(SaleCondition == "Normal", "normal", "other"),
         PorchSqFt = ScreenPorch + X3SsnPorch + EnclosedPorch + OpenPorchSF + WoodDeckSF,
         NewHome = ifelse(SaleType == 'New', 'new', 'other'))
test.impute$SalePrice = exp(predict(model4, test.impute))
head(test.impute)
```

## Appendices
