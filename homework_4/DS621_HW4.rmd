---
title: "Homework 4"
subtitle: "Insurance Linear and Binary Regression"
author: "Group 2"
date: "4/21/2021"
output:
  
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
---

**Group 2 members:** _Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, include = TRUE)

# Load required libraries.
library(tidyverse)
library(caret)
library(pROC)
library(grid)
library(Amelia)
library(ggplot2)
library(kableExtra)
library(corrplot)
library(reshape2)
library(e1071)
library(visdat)
```


## Assignment Overview

In this homework assignment, you will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.

Your objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

![variable information](./variable_information.png)

## Deliverables

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned predictions (probabilities, classifications, cost) for the evaluation data set. Use 0.5 threshold.
* Include your R statistical programming code in an Appendix.


## Task 1: Data Exploration

**Describe the size and the variables in the insurance training data set.**

```{r, dataExploration, echo=FALSE}
# Pull in the provided insurance training and evaluation datasets.
training_set <- read.csv('https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/homework_4/insurance_training_data.csv')
evaluation_set <- read.csv('https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/homework_4/insurance-evaluation-data.csv')

# Explore the structure of the training dataset.
str(training_set)

# Remove the INDEX colunmn as it is of no value.
training_set <- training_set[,setdiff(colnames(training_set), 'INDEX')]
```

\ 

Todo: Observation text goes here.

\clearpage

### Data Transformation

```{r, dataTransformationSpecialCharacters, echo=FALSE}
training_set
```


As we can see from the above table, The training dataset contains characters that will hinder our calculations so we need to remove or transform these. We will remove dollar signs from the INCOME, HOME_VAL, BLUEBOOK, and OLDCLAIM columns, and transform spaces to underscores in the EDUCATION, JOB, CAR_TYPE, URBANICITY columns.  

```{r, dataExplorationTransformation, echo=FALSE}
# Remove dollar signs from the INCOME, HOME_VAL, BLUEBOOK, and OLDCLAIM columns by replacing them with empty strings.
training_set$INCOME <- as.numeric(gsub("[\\$,]", "", training_set$INCOME))
training_set$HOME_VAL <- as.numeric(gsub("[\\$,]", "", training_set$HOME_VAL))
training_set$BLUEBOOK <- as.numeric(gsub("[\\$,]", "", training_set$BLUEBOOK))
training_set$OLDCLAIM <- as.numeric(gsub("[\\$,]", "", training_set$OLDCLAIM))

# Convert spaces to underscores in the EDUCATION, JOB, CAR_TYPE, and URBANICITY columns.
training_set$EDUCATION <- gsub(" ", "_", training_set$EDUCATION)
training_set$JOB <- gsub(' ', '_', training_set$JOB)
training_set$CAR_TYPE <- gsub(' ', '_', training_set$CAR_TYPE) 
training_set$URBANICITY <- gsub(' ', '_', training_set$URBANICITY)

# Remove '<', and 'z_' characters from the MSTATUS, SEX, EDUCATION, JOB, CAR_TYPE, and URBANICITY columns.  
training_set$MSTATUS <- gsub("[<,z_]", "", training_set$MSTATUS)
training_set$SEX <- gsub("[<,z_]", "", training_set$SEX)
training_set$EDUCATION <- gsub("[<,z_]", "", training_set$EDUCATION)
training_set$JOB <- gsub("[<,z_]", "", training_set$JOB)
training_set$CAR_TYPE <- gsub("[<,z_]", "", training_set$CAR_TYPE) 
training_set$URBANICITY <- gsub("[<,z_]", "", training_set$URBANICITY)

training_set
```

### Summary Statistics

Todo: Summary intro text here.

```{r, dataExplorationSummary, echo=FALSE}
# Summarize the training dataset.
summary(training_set)
```

\ 


Todo: Observation text here.


\clearpage

### Missing Values

```{r, dataExplorationMissingValues}
sapply(training_set, function(x) sum(is.na(x))) %>% kable() %>% kable_styling()
```

\ 

As we can see from the above table of missing values, five variables contain missing values. The "CAR_AGE" variable has the largest amount of missing variables (510), followed by "HOME_VAL" (464), YOJ" (454), "Income" (445), and finally "AGE" (6).

Below is a visual representation of the missing data. 

```{r, dataExplorationMissingValuesVisual}
vis_miss(training_set)
```


\clearpage

### Distributions

Having established that there are no missing values in the dataset, we will now take a look at the distribution profiles for each of the predictor variables. This will help us to decide which variables we should include in our final models.

```{r, predictorsDistributions, fig.height = 10, fig.width = 10, echo = FALSE}
# Using the Dplyr package, massage the data by removing the target value prior
# to plotting a histogram for each predictor variable.
predictor_vars <- training_set %>% dplyr::select(-TARGET_FLAG) %>%
  gather(key = 'predictor_variable', value = 'value')

# Plot and print a histogram for each predictor variable.
predictor_variables_plot <- ggplot(predictor_vars) +
  geom_histogram(aes(x = value, y = ..density..), bins = 30, color = 'blue') +
  labs(title = 'Distributions of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)

print(predictor_variables_plot)
```

\ 

Looking at the above distribution plots, we observe that there are a lot of skewed variables. Specifically, the `age` and `ptratio` variables are left skewed whilst the `dis`, `lstat`, `nox`, and `zn` variables are right skewed. The distance to employment centers (`dis`) variable tends to be lower and more right-skewed. The `chas` variable is a binary variable and therefore we only see values for 0.00 and 1.00.

\clearpage

### Box Plots

We used box plots to provide a visual insight into the spread of each predictor variable.

\ 

```{r, boxPlots, fig.height = 10, fig.width = 10, echo = FALSE}
# Create box plots for each of the predictor variables.
predictor_vars_boxplots <- training_set %>% dplyr::select(-TARGET_FLAG) %>%
  gather(key = 'variable', value = 'value') %>%
  ggplot(., aes(x = variable, y = value)) + 
  geom_boxplot(fill = 'salmon', color = 'darkred') +
  facet_wrap(~variable, scales = 'free', ncol = 4) +
  labs(x = element_blank(), y = element_blank(), title = 'Box Plots of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5))

print(predictor_vars_boxplots)
```

\ 

Todo: Observation text here.

\clearpage

### Correlations

```{r, dataCorrelation, echo=FALSE}
cor_table <- cbind(training_set[13], training_set[1:12]) %>% data.frame()
correlation_table <- cor(cor_table, method = 'pearson', use = 'complete.obs')[,1]
correlation_table %>%
  as.data.frame() %>%
  arrange(desc(abs(.))) %>%
  kable(caption = 'Correlation of Crime Rate Above Median') %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```


```{r, trainingDataCorrelation, echo=FALSE}
correlation_matrix <- training_set
correlation_matrix %>%
  cor(.) %>%
  corrplot(.,
           title = 'Correlation Matrix of Training Set Predictor Variables',
           method = 'color',
           type = 'lower',
           tl.col = 'black',
           tl.srt = 45,
           mar = c(0, 0, 2, 0))
```

\ 

According to the correlation table and plot above, there is a high correlation between the accessibility
to radial highways (`rad`), and the full-value property tax rate per $10,000 (`tax`) predictor variables. Additionally, the weighted 
means of distance to the five Boston employment centers (`dis`) variable is usually negatively correlated with the other variables.

\clearpage


### Variable Plots

Scatter plots of each variable versus the target variable.

```{r scatterPlots,fig.show = "hold", out.width="33%", echo=FALSE}
# Scatter plots for each of the variables against the target.
col_size = dim(training_set)[2]
cols = names(training_set)
for (col in cols[1:col_size-1]) {
  plot = training_set %>%
    ggplot(aes_string(x = col, y = 'target')) +
    geom_point(stat = 'identity') +
    labs(title = paste(col,'vs.','target'))

    print(plot)
}

```


## Task 2: Data Preparation

**Describe how you have transformed the data by changing the original variables or creating new variables.**

There are no missing values in the dataset so there is no need to impute values. Variable transformations (such as log, square root, quadratic, inverse, etc) will be applied during model building.

\clearpage

## Task 3: Build Models

Using the training data, build at least three different binary logistic regression models, using different variables (or the same variables with different transformations).

#### Model 1


#### Model 2


#### Model 3 


#### Model 4


#### Model 5

\clearpage

## Task 4: Select Models

Decide on the criteria for selecting the best binary logistic regression model.

```{r binaryLogic, echo=FALSE}
# Function that creates a vector of binary values based on threshold.
to_binary=function(arr,thresh){
  binary = c()
  for (i in arr) {
    if (i >= thresh) {
      binary = c(binary, 1)
    }
    else {
      binary = c(binary, 0)
    }
  }
  return(binary)
}


# Predictions based on a threshhold of 0.5.
predictions = training_set[c('TARGET_FLAG')]
predictions$model1 = to_binary(predict(model1,type ='response'),0.5)
predictions$model2 = to_binary(predict(model2,type ='response'),0.5)
predictions$model3 = to_binary(predict(model3,type ='response'),0.5)

```


### Error Calculations

```{r predictionsOne, echo=FALSE}

predictions = predictions %>%
  mutate(target = as.factor(target),
         model1 = as.factor(model1),
         model2 = as.factor(model2),
         model3 = as.factor(model3)
         )
```

### Model 1 Confusion Matrix


```{r, echo = FALSE}
# Model 1.
confusionMatrix(predictions$model1,predictions$target)
```

### Model 2 Confusion Matrix

```{r, echo = FALSE}
# Model 2.
confusionMatrix(predictions$model2,predictions$target)
```

### Model 3 Confusion Matrix

```{r, echo = FALSE}
# Model 3.
confusionMatrix(predictions$model3,predictions$target)
```

\clearpage

### Model Comparison

```{r, echo = FALSE, message= FALSE}
accuracy <- function(df,col1,col2) {
  true = df[,col1]
  predict = df[,col2]
  # total events
  len = length(true)
  # total correct predictions
  correct = 0
  for (i in seq(len)){
    if (true[i] == predict[i]){
      correct = correct + 1
    }
  }
  # accuracy
  return (correct/len)
}

class_error_rate <- function(df,col1,col2) {
  true = df[,col1]
  predict = df[,col2]
  # total events
  len = length(true)
  # total errors
  error = 0
  for (i in seq(len)){
    if (true[i] != predict[i]){
      error = error + 1
    }
  }
  # error rate
  return (error/len)
}

precision <- function(col1, col2) {
  # Calculate the total number of true positives in the dataset.
  true_positive <- sum(col1 == 1 & col2 == 1)
  # Calculate the total number of false positives in the dataset.
  false_positive <- sum(col1 == 0 & col2 == 1)
  # Perform the precision calculation and round the result to 2 decimal places.
  prediction_precision <- true_positive / (true_positive + false_positive)
  return(prediction_precision)
}


sensitivity <- function(col1, col2) {
  
  true_positive <- sum(col1 == 1 & col2 == 1)
  false_negative <- sum(col1 == 1 & col2 == 0)
  
  sensitivity<- true_positive / (true_positive + false_negative)
   
  return(sensitivity)
} 

specificity <- function(col1, col2) {
  
  true_negative <- sum(col2 == 0 & col1 == 0)
  false_positive <- sum(col2 == 1 & col1 == 0)
  
  specificity <- true_negative / (true_negative + false_positive) 
  
  return(specificity)
}

f1_score <- function(col1, col2) {
  sens <- sensitivity(col1, col2)
  prec <- precision(col1, col2)
  f1 <- 2 * sens * prec / (prec+sens)
  return(f1)
}

roc_model1 <- roc(predictions$target, as.numeric(predictions$model1))
roc_model2 <- roc(predictions$target, as.numeric(predictions$model2))
roc_model3 <- roc(predictions$target, as.numeric(predictions$model3))


#accuracy
acc <- c(accuracy(predictions,'target','model1'), accuracy(predictions,'target','model2'),
         accuracy(predictions,'target','model3'), accuracy(predictions,'target','model4'))

#classification error rate
class_error <- c(class_error_rate(predictions,'target','model1'), class_error_rate(predictions,'target','model2'),
                 class_error_rate(predictions,'target','model3'), class_error_rate(predictions,'target','model4'))

#precision
prec <- c(precision(predictions$target, predictions$model1), precision(predictions$target, predictions$model2),
          precision(predictions$target, predictions$model3), precision(predictions$target, predictions$model4))

#specificity
spec <- c(specificity(predictions$target, predictions$model1), specificity(predictions$target, predictions$model2),
          specificity(predictions$target, predictions$model3), specificity(predictions$target, predictions$model4))

#sensitivity
sens <- c(sensitivity(predictions$target, predictions$model1), sensitivity(predictions$target, predictions$model2),
          sensitivity(predictions$target, predictions$model3), sensitivity(predictions$target, predictions$model4))

#f1 score
f1 <- c(f1_score(predictions$target, predictions$model1), f1_score(predictions$target, predictions$model2), 
        f1_score(predictions$target, predictions$model3), f1_score(predictions$target, predictions$model4))

#AUC
a_u_c <- c(auc(roc_model1), auc(roc_model2), auc(roc_model3), auc(roc_model4))

model_comparison <- rbind(acc, class_error, prec, spec, sens, f1, a_u_c) %>%
  as.data.frame() %>%
  magrittr::set_rownames(c('accuracy', 'classification error rate', 'precision', 'sensitivity', 
                 'specificity', 'F1 score', 'AUC')) %>%
  magrittr::set_colnames(c('Model 1', 'Model 2', 'Model 3', 'Model 4')) %>%
  round(., 4)

model_comparison
```


### Model of Choice


\clearpage

## Appendix

```
# =====================================================================================
# Load Required Libraries 
# =====================================================================================

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, include = TRUE)

# Load required libraries.
library(tidyverse)
library(caret)
library(pROC)
library(grid)
library(Amelia)
library(ggplot2)
library(kableExtra)
library(corrplot)
library(reshape2)


# =====================================================================================
# Load The Datasets and Look at the Structure of the Data
# =====================================================================================

# Pull in the provided crime training and evaluation datasets.
training_set <- read.csv('CUNY_DATA_621/main/homework3/crime-training-data_modified.csv')
evaluation_set <- read.csv('CUNY_DATA_621/main/homework3/crime-evaluation-data_modified.csv')

# List the structure of the training dataset.
str(training_set)


# =====================================================================================
# Summarize the Training Data
# =====================================================================================

# Summarize the training dataset.
summary(training_set)


# =====================================================================================
# Check for Missing Values
# =====================================================================================

# Check for missing values using the Amelia package's missmap() function.
missmap(training_set, main = 'Missing Values Vs. Observed Values')
```
\clearpage

```
# =====================================================================================
# Distribution Plots
# =====================================================================================

# Using the Dplyr package, massage the data by removing the target value prior
# to plotting a histogram for each predictor variable.
predictor_vars <- training_set %>% dplyr::select(-target) %>%
  gather(key = 'predictor_variable', value = 'value')

# Plot and print a histogram for each predictor variable.
predictor_variables_plot <- ggplot(predictor_vars) +
  geom_histogram(aes(x = value, y = ..density..), bins = 30, color = 'blue') +
  labs(title = 'Distributions of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)

print(predictor_variables_plot)


# =====================================================================================
# Box Plots
# =====================================================================================

# Create box plots for each of the predictor variables.
predictor_vars_boxplots <- training_set %>% dplyr::select(-target) %>%
  gather(key = 'variable', value = 'value') %>%
  ggplot(., aes(x = variable, y = value)) + 
  geom_boxplot(fill = 'salmon', color = 'darkred') +
  facet_wrap(~variable, scales = 'free', ncol = 4) +
  labs(x = element_blank(), y = element_blank(), title = 'Box Plots of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5))

print(predictor_vars_boxplots)

# =====================================================================================
# Data Correlation Table and Matrix Plot
# =====================================================================================

cor_table <- cbind(training_set[13], training_set[1:12]) %>% data.frame()
correlation_table <- cor(cor_table, method = 'pearson', use = 'complete.obs')[,1]
correlation_table %>%
  kable(caption = 'Correlation of Crime Rate Above Median') %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
correlation_matrix <- training_set
correlation_matrix %>%
  cor(.) %>%
  corrplot(.,
           title = 'Correlation Matrix of Training Set Predictor Variables',
           method = 'color',
           type = 'lower',
           tl.col = 'black',
           tl.srt = 45,
           mar = c(0, 0, 2, 0))

# =====================================================================================
# Scatter Plots of Each Variable Versus the Target Variable
# =====================================================================================

# Scatter plots for each of the variables against the target.
col_size = dim(training_set)[2]
cols = names(training_set)
for (col in cols[1:col_size-1]) {
  plot = training_set %>%
    ggplot(aes_string(x = col, y = 'target')) +
    geom_point(stat = 'identity') +
    labs(title = paste(col,'vs.','target'))

    print(plot)
}


# =====================================================================================
# Model One
# =====================================================================================

model1 <- glm(target ~  ., family = "binomial", data = training_set)

summary(model1)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)


# =====================================================================================
# Model Two
# =====================================================================================

model2 <- glm(target ~ zn + indus + chas + nox +  sqrt(age) + dis + rad + tax + ptratio + 
                sqrt(lstat) + medv, family = "binomial", data = training_set)

summary(model2)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)
```

\clearpage

```
# =====================================================================================
# Model Three
# =====================================================================================

model3 <- glm(target ~ zn  + chas + nox +  sqrt(age) + dis + rad + tax + ptratio + 
                 medv + I(rad/tax^2), family = "binomial", data = training_set)

summary(model3)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)


# =====================================================================================
# Model Four
# =====================================================================================

model4 = glm(target~.-chas,training_set,family = "binomial")

summary(model4)


# =====================================================================================
# Model Selection
# =====================================================================================

# Function that creates a vector of binary values based on threshold.
to_binary = function(arr,thresh) {
  binary = c()
  for (i in arr) {
    if (i >= thresh) {
      binary = c(binary, 1)
    }
    else {
      binary = c(binary, 0)
    }
  }
  return(binary)
}

# Predictions based on a threshhold of 0.5.
predictions = training_set[c('target')]
predictions$model1 = to_binary(predict(model1,type ='response'),0.5)
predictions$model2 = to_binary(predict(model2,type ='response'),0.5)
predictions$model3 = to_binary(predict(model3,type ='response'),0.5)
predictions$model4 = to_binary(predict(model4,type ='response'),0.5)
head(predictions)
```

\clearpage

```

# =====================================================================================
# Error Calculations
# =====================================================================================

predictions = predictions %>%
  mutate(target = as.factor(target),
         model1 = as.factor(model1),
         model2 = as.factor(model2),
         model3 = as.factor(model3),
         model4 = as.factor(model4)
         )

# Model 1.
confusionMatrix(predictions$model1,predictions$target)

# Model 2.
confusionMatrix(predictions$model2,predictions$target)

# Model 3.
confusionMatrix(predictions$model3,predictions$target)

# Model 4.
confusionMatrix(predictions$model4,predictions$target)
```

\clearpage

```

# =====================================================================================
# Model Comparison
# =====================================================================================

accuracy <- function(df,col1,col2) {
  true = df[,col1]
  predict = df[,col2]
  # total events
  len = length(true)
  # total correct predictions
  correct = 0
  for (i in seq(len)){
    if (true[i] == predict[i]){
      correct = correct + 1
    }
  }
  # accuracy
  return (correct/len)
}

class_error_rate <- function(df,col1,col2) {
  true = df[,col1]
  predict = df[,col2]
  # total events
  len = length(true)
  # total errors
  error = 0
  for (i in seq(len)){
    if (true[i] != predict[i]){
      error = error + 1
    }
  }
  # error rate
  return (error/len)
}

precision <- function(col1, col2) {
  # Calculate the total number of true positives in the dataset.
  true_positive <- sum(col1 == 1 & col2 == 1)
  # Calculate the total number of false positives in the dataset.
  false_positive <- sum(col1 == 0 & col2 == 1)
  # Perform the precision calculation and round the result to 2 decimal places.
  prediction_precision <- true_positive / (true_positive + false_positive)
  return(prediction_precision)
}


sensitivity <- function(col1, col2) {
  
  true_positive <- sum(col1 == 1 & col2 == 1)
  false_negative <- sum(col1 == 1 & col2 == 0)
  
  sensitivity<- true_positive / (true_positive + false_negative)
   
  return(sensitivity)
} 

specificity <- function(col1, col2) {
  
  true_negative <- sum(col2 == 0 & col1 == 0)
  false_positive <- sum(col2 == 1 & col1 == 0)
  
  specificity <- true_negative / (true_negative + false_positive) 
  
  return(specificity)
}

f1_score <- function(col1, col2) {
  sens <- sensitivity(col1, col2)
  prec <- precision(col1, col2)
  f1 <- 2 * sens * prec / (prec+sens)
  return(f1)
}

roc_model1 <- roc(predictions$target, as.numeric(predictions$model1))
roc_model2 <- roc(predictions$target, as.numeric(predictions$model2))
roc_model3 <- roc(predictions$target, as.numeric(predictions$model3))
roc_model4 <- roc(predictions$target, as.numeric(predictions$model4))


#accuracy
acc <- c(accuracy(predictions,'target','model1'),
         accuracy(predictions,'target','model2'),
         accuracy(predictions,'target','model3'),
         accuracy(predictions,'target','model4'))

#classification error rate
class_error <- c(class_error_rate(predictions,'target','model1'),
                 class_error_rate(predictions,'target','model2'),
                 class_error_rate(predictions,'target','model3'),
                 class_error_rate(predictions,'target','model4'))

#precision
prec <- c(precision(predictions$target, predictions$model1),
          precision(predictions$target, predictions$model2),
          precision(predictions$target, predictions$model3),
          precision(predictions$target, predictions$model4))

#specificity
spec <- c(specificity(predictions$target, predictions$model1),
          specificity(predictions$target, predictions$model2),
          specificity(predictions$target, predictions$model3),
          specificity(predictions$target, predictions$model4))

#sensitivity
sens <- c(sensitivity(predictions$target, predictions$model1),
          sensitivity(predictions$target, predictions$model2),
          sensitivity(predictions$target, predictions$model3),
          sensitivity(predictions$target, predictions$model4))

#f1 score
f1 <- c(f1_score(predictions$target, predictions$model1),
        f1_score(predictions$target, predictions$model2), 
        f1_score(predictions$target, predictions$model3),
        f1_score(predictions$target, predictions$model4))

#AUC
a_u_c <- c(auc(roc_model1), auc(roc_model2), auc(roc_model3), auc(roc_model4))

model_comparison <- rbind(acc, class_error, prec, spec, sens, f1, a_u_c) %>%
  as.data.frame() %>%
  set_rownames(c('accuracy', 'classification error rate', 'precision', 'sensitivity', 
                 'specificity', 'F1 score', 'AUC')) %>%
  set_colnames(c('Model 1', 'Model 2', 'Model 3', 'Model 4')) %>%
  round(., 4)
```
