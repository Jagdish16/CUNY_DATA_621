---
title: "Homework 3"
subtitle: "Crime Logistic Regression"
author: "Group 2"
date: "4/11/2021"
output:
  
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
---

**Group 2 members:** _Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, include = TRUE)

# Load required libraries.
library(tidyverse)
library(caret)
library(pROC)
library(grid)
library(Amelia)
library(ggplot2)
library(kableExtra)
library(corrplot)
```


## Assignment Overview

In this homework assignment, you will explore, analyze, and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

* `zn`: the proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
* `indus`: the proportion of non-retail business acres per suburb (predictor variable)
* `chas`: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
* `nox`: nitrogen oxides concentration (parts per 10 million) (predictor variable)
* `rm`: average number of rooms per dwelling (predictor variable)
* `age`: the proportion of owner-occupied units built prior to 1940 (predictor variable)
* `dis`: weighted mean of distances to five Boston employment centers (predictor variable)
* `rad`: index of accessibility to radial highways (predictor variable)
* `tax`: full-value property-tax rate per $10,000 (predictor variable)
* `ptratio`: pupil-teacher ratio by town (predictor variable)
* `lstat`: lower status of the population (percent) (predictor variable)
* `medv`: median value of owner-occupied homes in $1000s (predictor variable)
* `target`: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

\clearpage

## Deliverables

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned prediction (probabilities, classifications) for the evaluation data set. Use a 0.5 threshold.
* Include your R statistical programming code in an Appendix.

## Task 1: Data Exploration

**Describe the size and the variables in the crime training data set.**

```{r, dataExploration, echo=FALSE}
# Pull in the provided crime training and evaluation datasets.
training_set <- read.csv('https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/homework%203/crime-training-data_modified.csv')
evaluation_set <- read.csv('https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/homework%203/crime-evaluation-data_modified.csv')

# List the structure of the training dataset.
str(training_set)
```

Based on the above data structure summary, the provided dataset consists of 13 variables and 466 observations. With the exception of the
"chas" variable (which is a dummy variable), and the "target" variable, all of the variables are numeric. The target variable is a binary value with 1 indicating that a neighborhood's crime rate is above the average, and 0 indicating that it is below the average.

### Summary Statistics

The first step in our data analysis is to compile summary statistics for each of the variables in the provided dataset. This will allow us to better understand the data prior to building our models.

```{r, dataExplorationSummary, echo=FALSE}
# Summarize the training dataset.
summary(training_set)
```

Looking at the target variable in the above summary, we can see that around 49% of the neigborhoods in the study have above average crime rates. The summary also tells us that some of the variables may contain skewed distributions as they have means that are far from the median. The zn, and tax variables are examples of this observation.

\clearpage

### Missing Values

Now that we have a better understanding of the dataset, we can move on to check for missing values in the data. As we can see from the below missingness map, there are no missing values and therefore we do not need to impute any of the values.

```{r, dataExplorationVisulization, echo=FALSE}
# Check for missing values using the Amelia package's missmap() function.
missmap(training_set, main = 'Missing Values Vs. Observed Values')
```

### Distributions

Having established that there are no missing values in the dataset, we will now take a look at the distribution profiles for each of the predictor variables.

```{r, predictorsDistributions, fig.height = 10, fig.width = 10, echo = FALSE}
# Using the Dplyr package, massage the data by removing the target value prior
# to plotting a histogram for each predictor variable.
predictor_vars <- training_set %>% dplyr::select(-target) %>%
  gather(key = 'predictor_variable', value = 'value')

# Plot and print a histogram for each predictor variable.
predictor_variables_plot <- ggplot(predictor_vars) +
  geom_histogram(aes(x = value, y = ..density..), bins = 30, color = 'blue') +
  labs(title = 'Distributions of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)

print(predictor_variables_plot)
```




## Task 2: Data Preparation

**Describe how you have transformed the data by changing the original variables or creating new variables.**

There are no missing values in the dataset so there is no need to impute values. Instead, we will split the training dataset (crime-training-data_modified.csv) into training and test sets using a 70/30 split respectively.

```{r, dataPreperationSplitData, echo=FALSE}
# Split the training data into training and test datasets using a 70/30 split ratio.
training_index <- createDataPartition(training_set$target,
                                      p = .7,
                                      list = FALSE,
                                      times = 1)

# Define the training dataset.
training_dataset <- training_set[training_index,]

# Define the test dataset.
test_dataset <- training_set[-training_index,]
```

**Summary of predictor variables in the training dataset.**

```{r, dataPreperationTrainingDataExploration, echo=FALSE}
training_dataset %>%
  select(-target) %>%
  summary()
```


```{r, dataPreperationTrainingDataCorrelation, echo=FALSE}
training_dataset %>%
  cor(.) %>%
  corrplot(.,
           title = 'Correlation Matrix of Training Set Predictor Variables',
           method = 'color',
           type = 'lower',
           tl.col = 'black',
           tl.srt = 45,
           mar = c(0, 0, 2, 0))
```



## Task 3: Build Models

Using the training data, build at least three different binary logistic regression models, using different variables (or the same variables with different transformations).


## Task 4: Select Models

Decide on the criteria for selecting the best binary logistic regression model.


## Appendix
