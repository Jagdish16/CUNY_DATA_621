---
title: "Homework 3"
subtitle: "Crime Logistic Regression"
author: "Group 2"
date: "4/11/2021"
output:
  
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
---

**Group 2 members:** _Diego Correa, Jagdish Chhabria, Orli Khaimova, Richard Zheng, Stephen Haslett_.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, include = TRUE)

# Load required libraries.
library(tidyverse)
library(caret)
library(pROC)
library(grid)
library(Amelia)
library(ggplot2)
library(kableExtra)
library(corrplot)
library(reshape2)
```


## Assignment Overview

In this homework assignment, you will explore, analyze, and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set:

* `zn`: the proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
* `indus`: the proportion of non-retail business acres per suburb (predictor variable)
* `chas`: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
* `nox`: nitrogen oxides concentration (parts per 10 million) (predictor variable)
* `rm`: average number of rooms per dwelling (predictor variable)
* `age`: the proportion of owner-occupied units built prior to 1940 (predictor variable)
* `dis`: weighted mean of distances to five Boston employment centers (predictor variable)
* `rad`: index of accessibility to radial highways (predictor variable)
* `tax`: full-value property-tax rate per $10,000 (predictor variable)
* `ptratio`: pupil-teacher ratio by town (predictor variable)
* `lstat`: lower status of the population (percent) (predictor variable)
* `medv`: median value of owner-occupied homes in $1000s (predictor variable)
* `target`: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

\clearpage

## Deliverables

* A write-up submitted in PDF format. Your write-up should have four sections. Each one is described below. You may assume you are addressing me as a fellow data scientist, so do not need to shy away from technical details.
* Assigned prediction (probabilities, classifications) for the evaluation data set. Use a 0.5 threshold.
* Include your R statistical programming code in an Appendix.

## Task 1: Data Exploration

**Describe the size and the variables in the crime training data set.**

```{r, dataExploration, echo=FALSE}
# Pull in the provided crime training and evaluation datasets.
training_set <- read.csv('https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/homework%203/crime-training-data_modified.csv')
evaluation_set <- read.csv('https://raw.githubusercontent.com/Jagdish16/CUNY_DATA_621/main/homework%203/crime-evaluation-data_modified.csv')

# List the structure of the training dataset.
str(training_set)
```

\ 

Based on the above data structure summary, the provided dataset consists of 13 variables and 466 observations. With the exception of the
"chas" variable (which is a dummy variable), and the "target" variable, all of the variables are numeric.

The target variable is a binary value with 1 indicating that a neighborhood's crime rate is above the average, and 0 indicating that it is below the average.

\clearpage

### Summary Statistics

The first step in our data analysis is to compile summary statistics for each of the variables in the provided dataset. This will allow us to better understand the data prior to building our models.

```{r, dataExplorationSummary, echo=FALSE}
# Summarize the training dataset.
summary(training_set)
```

\ 

Looking at the target variable in the above summary, we can see that around 49% of the neighborhoods in the study have above average crime rates.

The summary also tells us that some of the variables may contain skewed distributions as they have means that are far from the median. The zn, and tax variables are examples of this observation. We will verify whether this is the case or not in the "Distributions" section.

\clearpage

### Missing Values

Now that we have a better understanding of the dataset, we can move on to check for missing values in the data.

```{r, dataExplorationVisulization, fig.height = 10, fig.width = 10, echo = FALSE}
# Check for missing values using the Amelia package's missmap() function.
missmap(training_set, main = 'Missing Values Vs. Observed Values')
```

\ 

As we can see from the above missingness map, there are no missing values and therefore we do not need to impute any of the values to account for this.

\clearpage

### Distributions

Having established that there are no missing values in the dataset, we will now take a look at the distribution profiles for each of the predictor variables. This will help us to decide which variables we should include in our final models.

```{r, predictorsDistributions, fig.height = 10, fig.width = 10, echo = FALSE}
# Using the Dplyr package, massage the data by removing the target value prior
# to plotting a histogram for each predictor variable.
predictor_vars <- training_set %>% dplyr::select(-target) %>%
  gather(key = 'predictor_variable', value = 'value')

# Plot and print a histogram for each predictor variable.
predictor_variables_plot <- ggplot(predictor_vars) +
  geom_histogram(aes(x = value, y = ..density..), bins = 30, color = 'blue') +
  labs(title = 'Distributions of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)

print(predictor_variables_plot)
```

\ 

Looking at the above distribution plots, we observe that there are a lot of skewed variables. Specifically, the "age", and "ptratio" variables are left skewed whilst the "dis", "lstat", "nox", and "zn" variables are right skewed. The distance to employment centers (dis) variable tends to be lower and more right-skewed. The "chas" variable is a binary variable and therefore we only see values for 0.00 and 1.00.

\clearpage

### Box Plots

We used box plots to provide a visual insight into the spread of each predictor variable.

\ 

```{r, boxPlots, fig.height = 10, fig.width = 10, echo = FALSE}
# Create box plots for each of the predictor variables.
predictor_vars_boxplots <- training_set %>% dplyr::select(-target) %>%
  gather(key = 'variable', value = 'value') %>%
  ggplot(., aes(x = variable, y = value)) + 
  geom_boxplot(fill = 'salmon', color = 'darkred') +
  facet_wrap(~variable, scales = 'free', ncol = 4) +
  labs(x = element_blank(), y = element_blank(), title = 'Box Plots of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5))

print(predictor_vars_boxplots)
```

\ 

The box plots show that some variables have a large amount of variance between each other (i.e. rad, tax, and zn). They also show significant outliers for some of the variables.

\clearpage


```{r, dataCorrelation, echo=FALSE}
cor_table <- cbind(training_set[13], training_set[1:12]) %>% data.frame()
correlation_table <- cor(cor_table, method = 'pearson', use = 'complete.obs')[,1]
correlation_table %>%
  kable(caption = 'Correlation of Crime Rate Above Median') %>% kable_styling(bootstrap_options = c("striped", "hover"))
```


```{r, trainingDataCorrelation, echo=FALSE}
correlation_matrix <- training_set
correlation_matrix %>%
  cor(.) %>%
  corrplot(.,
           title = 'Correlation Matrix of Training Set Predictor Variables',
           method = 'color',
           type = 'lower',
           tl.col = 'black',
           tl.srt = 45,
           mar = c(0, 0, 2, 0))
```

\ 

According to the correlation table and plot above, there is a high correlation between the accessibility
to radial highways (rad), and the full-value property tax rate per $10,000 (tax) predictor variables. Additionally, the weighted 
means of distance to the five Boston employment centers (dis) variable is usually negatively correlated
with the other variables.

\clearpage

### Variable Plots

Scatter plots of each variable versus the target variable.

```{r scatterPlots, echo=FALSE}
# Scatter plots for each of the variables against the target.
col_size = dim(training_set)[2]
cols = names(training_set)
for (col in cols[1:col_size-1]) {
  plot = training_set %>%
    ggplot(aes_string(x = col, y = 'target')) +
    geom_point(stat = 'identity') +
    labs(title = paste(col,'vs.','target'))

    print(plot)
}

```
\clearpage

## Task 2: Data Preparation

**Describe how you have transformed the data by changing the original variables or creating new variables.**

There are no missing values in the dataset so there is no need to impute values. Variable transformations (such as log, square root, quadratic, inverse, etc) will be applied during model building.


## Task 3: Build Models

Using the training data, build at least three different binary logistic regression models, using different variables (or the same variables with different transformations).

#### Model 1

This model uses all of the variables and acts as a guide to which variables need to be included, excluded, or transformed.
The `nox` variable has the greatest affect on the target variable, but the coefficients do not make sense as the intercept is out of bounds.

```{r modelOne, fig.show="hold", out.width="50%", echo=FALSE}
model1 <- glm(target ~  ., family = "binomial", data = training_set)

summary(model1)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)
```

#### Model 2

- Log/sqrt was applied to `age` and `lstat` as they were skewed.
- `rm` was removed since it had a high p value.

```{r modelTwo, fig.show = "hold", out.width="50%", echo=FALSE}
model2 <- glm(target ~ zn + indus + chas + nox +  sqrt(age) + dis + rad + tax + ptratio + 
                sqrt(lstat) + medv, family = "binomial", data = training_set)

summary(model2)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)
```

#### Model 3 

- Log/sqrt was applied to `age` and `stat` as they were skewed.
- `rm` was removed since it had a high p value.
- `lstat` was removed due to high p value
- ratio of `rad`/`tax`, the full value property tax value squared per index of accessibility to radial highways
- `indus` was removed 

```{r modelThree, fig.show = "hold", out.width="50%", echo=FALSE}
model3 <- glm(target ~ zn  + chas + nox +  sqrt(age) + dis + rad + tax + ptratio + 
                 medv + I(rad/tax^2), family = "binomial", data = training_set)

summary(model3)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)
```

#### Model 4

- remove chas variable because it is a binary variable

```{r chasVariableRemove,  echo=FALSE}
model4 = glm(target~.-chas,training_set,family = "binomial")

summary(model4)
```

\clearpage

## Task 4: Select Models

Decide on the criteria for selecting the best binary logistic regression model.

```{r binaryLogic, echo=FALSE}
# Function that creates a vector of binary values based on threshold.
to_binary=function(arr,thresh){
  binary = c()
  for (i in arr) {
    if (i >= thresh) {
      binary = c(binary, 1)
    }
    else {
      binary = c(binary, 0)
    }
  }
  return(binary)
}

# Predictions based on a threshhold of 0.5.
predictions = training_set[c('target')]
predictions$model1 = to_binary(predict(model1,type ='response'),0.5)
predictions$model2 = to_binary(predict(model2,type ='response'),0.5)
predictions$model3 = to_binary(predict(model3,type ='response'),0.5)
predictions$model4 = to_binary(predict(model4,type ='response'),0.5)
head(predictions)

```

#### Error Calculations

```{r predictionsOne, echo=FALSE}
predictions = predictions %>%
  mutate(target = as.factor(target),
         model1 = as.factor(model1),
         model2 = as.factor(model2),
         model3 = as.factor(model3),
         model4 = as.factor(model4)
         )

# Model 1.
confusionMatrix(predictions$model1,predictions$target)

# Model 2.
confusionMatrix(predictions$model2,predictions$target)

# Model 3.
confusionMatrix(predictions$model3,predictions$target)

# Model 4.
confusionMatrix(predictions$model4,predictions$target)

```

#### Since model 4 has the highest sensitivity (low false negative) rate, we will be picking this model to predict on the evaluation set.

```{r predictionsTwo, echo=FALSE}
evaluation_set$predictions = to_binary(predict(model4, evaluation_set,type = 'response'), 0.5)

head(evaluation_set)
```

\clearpage

## Appendix

```
# =====================================================================================
# Load Required Libraries 
# =====================================================================================

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, include = TRUE)

# Load required libraries.
library(tidyverse)
library(caret)
library(pROC)
library(grid)
library(Amelia)
library(ggplot2)
library(kableExtra)
library(corrplot)
library(reshape2)


# =====================================================================================
# Load The Datasets and Look at the Structure of the Data
# =====================================================================================

# Pull in the provided crime training and evaluation datasets.
training_set <- read.csv('CUNY_DATA_621/main/homework3/crime-training-data_modified.csv')
evaluation_set <- read.csv('CUNY_DATA_621/main/homework3/crime-evaluation-data_modified.csv')

# List the structure of the training dataset.
str(training_set)


# =====================================================================================
# Summarize the Training Data
# =====================================================================================

# Summarize the training dataset.
summary(training_set)


# =====================================================================================
# Check for Missing Values
# =====================================================================================

# Check for missing values using the Amelia package's missmap() function.
missmap(training_set, main = 'Missing Values Vs. Observed Values')

# =====================================================================================
# Distribution Plots
# =====================================================================================

# Using the Dplyr package, massage the data by removing the target value prior
# to plotting a histogram for each predictor variable.
predictor_vars <- training_set %>% dplyr::select(-target) %>%
  gather(key = 'predictor_variable', value = 'value')

# Plot and print a histogram for each predictor variable.
predictor_variables_plot <- ggplot(predictor_vars) +
  geom_histogram(aes(x = value, y = ..density..), bins = 30, color = 'blue') +
  labs(title = 'Distributions of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)

print(predictor_variables_plot)


# =====================================================================================
# Box Plots
# =====================================================================================

# Create box plots for each of the predictor variables.
predictor_vars_boxplots <- training_set %>% dplyr::select(-target) %>%
  gather(key = 'variable', value = 'value') %>%
  ggplot(., aes(x = variable, y = value)) + 
  geom_boxplot(fill = 'salmon', color = 'darkred') +
  facet_wrap(~variable, scales = 'free', ncol = 4) +
  labs(x = element_blank(), y = element_blank(), title = 'Box Plots of Predictor Variables') +
  theme(plot.title = element_text(hjust = 0.5))

print(predictor_vars_boxplots)

# =====================================================================================
# Data Correlation Table and Matrix Plot
# =====================================================================================

cor_table <- cbind(training_set[13], training_set[1:12]) %>% data.frame()
correlation_table <- cor(cor_table, method = 'pearson', use = 'complete.obs')[,1]
correlation_table %>%
  kable(caption = 'Correlation of Crime Rate Above Median') %>% kable_styling(bootstrap_options = c("striped", "hover"))
  
correlation_matrix <- training_set
correlation_matrix %>%
  cor(.) %>%
  corrplot(.,
           title = 'Correlation Matrix of Training Set Predictor Variables',
           method = 'color',
           type = 'lower',
           tl.col = 'black',
           tl.srt = 45,
           mar = c(0, 0, 2, 0))

# =====================================================================================
# Scatter Plots of Each Variable Versus the Target Variable
# =====================================================================================

# Scatter plots for each of the variables against the target.
col_size = dim(training_set)[2]
cols = names(training_set)
for (col in cols[1:col_size-1]) {
  plot = training_set %>%
    ggplot(aes_string(x = col, y = 'target')) +
    geom_point(stat = 'identity') +
    labs(title = paste(col,'vs.','target'))

    print(plot)
}


# =====================================================================================
# Model One
# =====================================================================================

model1 <- glm(target ~  ., family = "binomial", data = training_set)

summary(model1)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)


# =====================================================================================
# Model Two
# =====================================================================================

model2 <- glm(target ~ zn + indus + chas + nox +  sqrt(age) + dis + rad + tax + ptratio + 
                sqrt(lstat) + medv, family = "binomial", data = training_set)

summary(model2)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)


# =====================================================================================
# Model Three
# =====================================================================================

model3 <- glm(target ~ zn  + chas + nox +  sqrt(age) + dis + rad + tax + ptratio + 
                 medv + I(rad/tax^2), family = "binomial", data = training_set)

summary(model3)

plot(fitted(model1), resid(model1), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model1), col = "dodgerblue", lwd = 2)


# =====================================================================================
# Model Four
# =====================================================================================

model4 = glm(target~.-chas,training_set,family = "binomial")

summary(model4)


# =====================================================================================
# Model Selection
# =====================================================================================

# Function that creates a vector of binary values based on threshold.
to_binary = function(arr,thresh) {
  binary = c()
  for (i in arr) {
    if (i >= thresh) {
      binary = c(binary, 1)
    }
    else {
      binary = c(binary, 0)
    }
  }
  return(binary)
}

# Predictions based on a threshhold of 0.5.
predictions = training_set[c('target')]
predictions$model1 = to_binary(predict(model1,type ='response'),0.5)
predictions$model2 = to_binary(predict(model2,type ='response'),0.5)
predictions$model3 = to_binary(predict(model3,type ='response'),0.5)
predictions$model4 = to_binary(predict(model4,type ='response'),0.5)
head(predictions)


# =====================================================================================
# Error Calculations
# =====================================================================================

predictions = predictions %>%
  mutate(target = as.factor(target),
         model1 = as.factor(model1),
         model2 = as.factor(model2),
         model3 = as.factor(model3),
         model4 = as.factor(model4)
         )

# Model 1.
confusionMatrix(predictions$model1,predictions$target)

# Model 2.
confusionMatrix(predictions$model2,predictions$target)

# Model 3.
confusionMatrix(predictions$model3,predictions$target)

# Model 4.
confusionMatrix(predictions$model4,predictions$target)
```
