---
title: "Data621 Homework One"
author: "Stephen Haslett"
date: "2/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Disable scientific numbers.
options(scipen = 999)

# Load required libraries.
library(skimr)
library(knitr)
library(ggplot2)
library(tidyverse)
library(kableExtra)
library(caret)
library(Hmisc)
library(gtsummary)
library(rsample)
library(MASS)
```



```{r dataImport}
# Pull in the training data.
mb_training_data <- read.csv('moneyball-training-data.csv', header = TRUE)

# Remove the INDEX variable as it is of no value in the data evaluation.
mb_training_data <- mb_training_data %>% 
  dplyr::select(-INDEX)
```

### Summary Stats

```{r dataExploration}
# Remove the prefix "TEAM_" from variable names to make them more readable.
names(mb_training_data) <- names(mb_training_data) %>% 
    str_replace_all('TEAM_', '')

# Summarize the data.
summary(mb_training_data)
```


### Variable Distributions

```{r dataDistributions, fig.height = 10, fig.width = 10, error=FALSE, warning=FALSE}
distribution_data <- mb_training_data %>%
  gather(key = 'variable_name', value = 'value')

ggplot(distribution_data) + 
  geom_histogram(aes(x = value, y = ..density..), bins = 30) + 
  geom_density(aes(x = value), color = 'red') +
  facet_wrap(. ~ variable_name, scales = 'free', ncol = 4)
```


### Missing Data

```{r missingDataPercentages, echo=FALSE}
# Create a table of variables sorted by percentage of missing data. 
missing_data <- colSums(mb_training_data %>% sapply(is.na))
percentage_missing <- round(missing_data / nrow(mb_training_data) * 100, 2)
missing_values_table <- sort(percentage_missing, decreasing = TRUE)

missing_values_table %>%
  kable(caption = 'Breakdown of Variables by Percentage of Missing Data') %>%
  kable_styling()
```


91.61% percent of the rows are missing from the BATTING_HP variable, so we will remove this variable from the dataset completely. The percentage of missing data for the remaining variables with missing data is much less, and so excluding them from the final model could skew the results. Therefore, rather than dropping the variables completely, we will impute the missing values with the median value of the variable column in question.

```{r missingDataManipulation, echo=FALSE}
# Drop the BATTING_HBP variable from the dataset.
mb_training_data <- mb_training_data %>% 
  dplyr::select(-BATTING_HBP)
```

**Snapshot of the data prior to mean imputation**

```{r missingDataPreImputationSnapshot, echo=FALSE}
# take a look at the data prior to imputation.
head(mb_training_data)
```


```{r missingDataManipulationImputation, echo=FALSE}
# Perform mean imputation on the remaining variables with missing values.
for(i in 1:ncol( mb_training_data)) {
   mb_training_data[ , i][is.na(mb_training_data[ , i])] <- mean(mb_training_data[ , i], na.rm = TRUE)
}
```

**Snapshot of the data after mean imputation**

```{r missingDataPostImputationSnapshot, echo=FALSE}
# Look at the data after imputation.
head(mb_training_data)
```


#### Correlation Table

```{r correlationTable}
# Perform a correlation analysis on the data. In this analysis, we are only interested in the
# correlation of the predicter variables and the "TARGET_WINS" variable.
correlation_table <- cor(mb_training_data, method = 'pearson', use = 'complete.obs')[,1]

# Remove the TARGET_WINS variable from the correlation table as it is redundant
# within the context of of our correlation analysis.
correlation_table <- correlation_table[-c(1)]

correlation_table %>%
  kable(caption = 'Correlation of Variables to TARGET_WINS') %>%
  kable_styling()
```



### Data Models

**Split the training dataset in two using an 80/20 ratio - mb_training_dataset, mb_testing_dataset**.

```{r dataSplit}
set.seed(4)
mb_split_data <- initial_split(mb_training_data, 0.8)
mb_training_dataset <- training(mb_split_data)
mb_testing_dataset <- testing(mb_split_data)
```

#### Model 1

Model 1 includes all variables in the dataset.

```{r dataModelOne}
model_one <- lm(TARGET_WINS ~ BATTING_H + BATTING_2B + BATTING_3B +
                BATTING_HR + BATTING_BB + BATTING_SO + BASERUN_SB +
                BASERUN_CS + PITCHING_H + PITCHING_HR + PITCHING_BB +
                PITCHING_SO + FIELDING_E + FIELDING_DP,
                mb_training_dataset)
```


##### Model 1 Statistics

**Model 1 Summary Stats**

```{r modelOneSummaryStats, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
summary(model_one)
```


**Model 1 R Squared**
```{r modelOneRSquared, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
summary(model_one)$r.squared
```



**Model 1 Confidence Intervals**
```{r modelOneConfidenceIntervals, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
confint(model_one)
```

##### Model 1 Plots

```{r modelOnePlots, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
plot(fitted(model_one), resid(model_one), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
qqnorm(resid(model_one), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_one), col = "dodgerblue", lwd = 2)
```

#### Model 2

Model 2 - Uses stepwise regression on the variables in Model 1 to create the best performing model.

```{r dataModelTwo}
model_two <- stepAIC(model_one, direction = 'both', trace = FALSE)
```


**Model 2 Summary Stats**

```{r modelTwoSummaryStats, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
summary(model_two)
```


**Model 2 R Squared**
```{r modelTwoRSquared, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
summary(model_two)$r.squared
```


**Model 2 Confidence Intervals**
```{r modelTwoConfidenceIntervals, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
confint(model_two)
```


##### Model 2 Plots

```{r modelTwoPlots, error=FALSE, warning=FALSE, echo=FALSE, message=FALSE}
plot(fitted(model_two), resid(model_two), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_two), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_two), col = "dodgerblue", lwd = 2)
```


